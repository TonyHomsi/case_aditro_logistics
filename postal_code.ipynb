{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Case Data Scientist\n",
    "\n",
    "In this notebook, we're going to go through an example machine learning project with the goal of predict the number of transports that may start in a given postal code of five digits on a certain day of the year.\n",
    "\n",
    "Problem 1:\n",
    "\n",
    "The first problem will be solved using the following steps:\n",
    "1. Problem definition.\n",
    "2. Data.\n",
    "3. EDA.\n",
    "4. Preprocessing\n",
    "5. Model\n",
    "6. Evaluating the Model\n",
    "7. Feature Important\n",
    "\n",
    "Problem 2:\n",
    "\n",
    "In this problem, we'll answer ***some*** critical questoions to get more insights that improve business decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: Data Science\n",
    "\n",
    "## 1. problem definition\n",
    "\n",
    "> Developing a Machine Learning model that is able to **predict the number of transports** that may start in a given postal code of five digits on a certain day of the year.\n",
    "\n",
    "## 2. Data\n",
    "\n",
    "There are 2 main datasets:\n",
    "\n",
    "* **Innrikes Paket kost 2019.09 - 2020.08.pickle** \n",
    "  * Dataset of transports within Sweden.\n",
    "  * The end and start of the journey for the marchandise is described by the Sweden postal zip code with 5 digits (more granular than the one with 3 digits)\n",
    "  * Also included are the start date of the transport, and various KPIs, such as the duration, weight, volume, and cost.\n",
    "  \n",
    "* **df_postal_code_sweden.pickle:**\n",
    "  presents the latitude and longitude of one point (maybe the center) within each postal code of 5 digits in Sweden.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import pickle\n",
    "\n",
    "from zipfile import ZipFile\n",
    "import sys\n",
    "\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "\n",
    "sys.path.append('../')\n",
    "from handling_missing_data import CleanData\n",
    "# to impute missing data with Feature-engine:\n",
    "from feature_engine.imputation import RandomSampleImputer\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error,r2_score\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import files\n",
    "\n",
    "df_postal_code_sweden = pickle.load(open('./Data/df_postal_code_sweden.pickle', 'rb'))\n",
    "gdf_geocode_sweden = pickle.load(open('./Data/gdf_geocode_sweden.pickle', 'rb'))\n",
    "\n",
    "with ZipFile('./Data/Innrikes Paket kost 2019.09 - 2020.08.pickle.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('./Data/')\n",
    "df_Innrikes_Paket_kost = pickle.load(open('./Data/Innrikes Paket kost 2019.09 - 2020.08.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert file Pickle to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the pickle files to csv files\n",
    "df_postal_code_sweden.to_csv(r'./Data/df_postal_code_sweden.csv',index=False)\n",
    "gdf_geocode_sweden.to_csv(r'./Data/gdf_geocode_sweden.csv',index=False)\n",
    "df_Innrikes_Paket_kost.to_csv(r'./Data/df_Innrikes_Paket_kost.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read file as csv\n",
    "df_postal_code_sweden = pd.read_csv('./Data/df_postal_code_sweden.csv')\n",
    "gdf_geocode_sweden = pd.read_csv('./Data/gdf_geocode_sweden.csv')\n",
    "df_Innrikes_Paket_kost = pd.read_csv('./Data/df_Innrikes_Paket_kost.csv', parse_dates=['DepartureDate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. EDA\n",
    "Exploratory Data Analysis (EDA) is the process of visualizing and analyzing data to extract insights from it. In other words, EDA is the process of summarizing important characteristics of data in order to gain better understanding of the dataset.\n",
    "Since EDA has no real set methodolgy, the following is a short check list you might want to walk through:\n",
    "\n",
    "1. What kind of data do you have and how do you treat different types?\n",
    "2. Whatâ€™s missing from the data and how do you deal with it?\n",
    "3. Where are the outliers and why should you care about them?\n",
    "4. How can you add, change or remove features to get more out of your data?\n",
    "- Count of unique values\n",
    "- Numeric columns\n",
    "- Missing values\n",
    "- Summary stats\n",
    "- Outliers:\n",
    "    - Considerably higher or lower\n",
    "    - Require further investigation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files dim\n",
    "df_postal_code_sweden.shape, gdf_geocode_sweden.shape, df_Innrikes_Paket_kost.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the first 5 rows\n",
    "df_postal_code_sweden.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_geocode_sweden.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Innrikes_Paket_kost.head()\n",
    "df_Innrikes_Paket_kost.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the type of variables\n",
    "df_postal_code_sweden.info()\n",
    "gdf_geocode_sweden.info()\n",
    "df_Innrikes_Paket_kost.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating some statistical data like percentile, mean and std of the numerical values\n",
    "df_Innrikes_Paket_kost.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find the percentage of missing values in each column\n",
    "def find_missing_percentage(df):\n",
    "    percent_missing = df.isnull().sum() * 100 / len(df)\n",
    "    missing_value_df = pd.DataFrame({'column_name': df.columns, 'percent_missing': percent_missing})\n",
    "    missing_value_df.sort_values('percent_missing', inplace=True)\n",
    "    return missing_value_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the missing values in the df_postal_code_sweden\n",
    "find_missing_percentage(df_postal_code_sweden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the missing values in the df_postal_code_sweden\n",
    "find_missing_percentage(gdf_geocode_sweden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_missing_percentage(df_Innrikes_Paket_kost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uniqueness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the different unique values of different columns\n",
    "df_Innrikes_Paket_kost.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the different unique values of different columns\n",
    "df_postal_code_sweden.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Innrikes_Paket_kost['PlaceOfDestination'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking the Numerical & Categorical variables including the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From Freezeframes select only the numeric cols \n",
    "numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "df_numerics = df_Innrikes_Paket_kost.select_dtypes(include=numerics)\n",
    "Col_num = df_Innrikes_Paket_kost.select_dtypes(include=np.number).columns.tolist()\n",
    "df_numerics.head()\n",
    "df_numerics.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_singular_matrix = []\n",
    "for i in range(len(Col_num)):\n",
    "    if len(df_Innrikes_Paket_kost[Col_num[i]].unique()) == 2:\n",
    "        print(f'Varable_name: {Col_num[i]}')\n",
    "        list_singular_matrix.append(Col_num[i])\n",
    "df_Innrikes_Paket_kost.drop(list_singular_matrix,inplace=True,axis=1)\n",
    "Col_num = df_Innrikes_Paket_kost.select_dtypes(include=np.number).columns.tolist()\n",
    "df_Innrikes_Paket_kost.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find categorical variables\n",
    "\n",
    "numerical = [var for var in df_Innrikes_Paket_kost.columns if df_Innrikes_Paket_kost[var].dtype!='O']\n",
    "\n",
    "print('There are {} numerical variables'.format(len(numerical)))\n",
    "\n",
    "# Check for columns which are numeric\n",
    "for label, content in df_Innrikes_Paket_kost.items():\n",
    "    if  pd.api.types.is_numeric_dtype(content):\n",
    "        print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_categorical = df_Innrikes_Paket_kost.select_dtypes(include= object)\n",
    "Col_cat = df_Innrikes_Paket_kost.select_dtypes(include=np.object_).columns.tolist()\n",
    "df_categorical.head()\n",
    "df_categorical.isnull().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find categorical variables\n",
    "\n",
    "categorical = [var for var in df_Innrikes_Paket_kost.columns if df_Innrikes_Paket_kost[var].dtype=='O']\n",
    "\n",
    "print('There are {} categorical variables'.format(len(categorical)))\n",
    "\n",
    "# Check for columns which aren't numeric\n",
    "for label, content in df_Innrikes_Paket_kost.items():\n",
    "    if not pd.api.types.is_numeric_dtype(content):\n",
    "        print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df_Innrikes_Paket_kost.DepartureDate, df_Innrikes_Paket_kost.PlaceOfDeparture)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sort DataFrame by DepartureDate\n",
    "\n",
    "As we're working on a time series problem and trying to predict future examples given past examples, it makes sense to sort our data by date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort DataFrame in date order\n",
    "df_Innrikes_Paket_kost.sort_values(by=[\"DepartureDate\"], inplace=True, ascending=True)\n",
    "df_Innrikes_Paket_kost.DepartureDate.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Innrikes_Paket_kost.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Report and Plotting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U dataprep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import DataPrep library\n",
    "import dataprep as dpr\n",
    "from dataprep.eda import plot\n",
    "from dataprep.eda import plot_missing\n",
    "from dataprep.eda import plot_correlation\n",
    "#from dataprep.eda import create_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot(df_Innrikes_Paket_kost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_missing(df_Innrikes_Paket_kost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_correlation(df_Innrikes_Paket_kost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Preprocessing\n",
    "\n",
    "**Steps to clean the data:**\n",
    " - Remove duplicated features in each file.\n",
    " - Remove Constant and Quasi Features.\n",
    " - Remove low/high correlated features.\n",
    " - Covnert the features to correct format and type.\n",
    " - Imputing the missing data in the numerical and categorical variables.\n",
    " - Removing Outliers.\n",
    " - Scaling/transfomr the data. (*Solved in the Model section*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make a copy of the original DataFrame\n",
    "\n",
    "Since we're going to be manipulating the data, we'll make a copy of the original DataFrame and perform our changes there.\n",
    "\n",
    "This will keep the original DataFrame in tact if we need it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = CleanData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the original DataFrame for preprocessing\n",
    "df_tmp = df_Innrikes_Paket_kost.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropping Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_remove_signMat_dup_cons_quas_row_col = clean_data.drop_const_quasi_dupl(df_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_remove_signMat_dup_cons_quas_row_col.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting Numerical <==> Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_conv_num_to_cat = [\"ToZone\", \"FromZone\"]\n",
    "col_conv_cat_to_num = ['GrossWeight']\n",
    "df_tmp = clean_data.convert_num_to_cat(df_remove_signMat_dup_cons_quas_row_col,col_conv_num_to_cat)\n",
    "df_tmp = clean_data.convert_cat_to_num(df_remove_signMat_dup_cons_quas_row_col,col_conv_cat_to_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_numerics = df_tmp.select_dtypes(include=numerics)\n",
    "Col_num = df_tmp.select_dtypes(include=np.number).columns.tolist()\n",
    "Col_cat = df_tmp.select_dtypes(include=np.object_).columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputing Missing Data\n",
    "\n",
    "After investigation the missing data in EDA section, we notice that the missing values are only in the categorical columns:\n",
    "- ToZone --> 0.679478%\n",
    "- FromZone --> 3.476942%\n",
    "- PlaceOfDestination --> 17.366724%\n",
    "- PlaceOfDeparture --> 20.131676%\n",
    "- ConsigneeName --> 23.878576%\n",
    "\n",
    "To impute missing data, we are going to use `The RandomSampleImputer()` from feature_engine to replace missing data( works both numerical and categorical ) with a random sample extracted from the variables in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install feature-engine\n",
    "imputer = RandomSampleImputer(variables=Col_cat)\n",
    "imputer.fit(df_tmp)\n",
    "df_tmp_impute = imputer.transform(df_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No missing values\n",
    "find_missing_percentage(df_tmp_impute)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the capper\n",
    "from feature_engine.outliers import Winsorizer\n",
    "windsoriser = Winsorizer(capping_method='iqr', # choose iqr for IQR rule boundaries or gaussian for mean and std\n",
    "                          tail='both', # cap left, right or both tails \n",
    "                          fold=3)\n",
    "windsoriser.fit(df_tmp_impute)\n",
    "df_remov_outliers = windsoriser.transform(df_tmp_impute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windsoriser.left_tail_caps_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windsoriser.right_tail_caps_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create histogram, Q-Q plot and\n",
    "# boxplot. We learned this in section 3 of the course\n",
    "import seaborn as sns\n",
    "# for Q-Q plots\n",
    "def diagnostic_plots(df, variable):\n",
    "    # function takes a dataframe (df) and\n",
    "    # the variable of interest as arguments\n",
    "\n",
    "    # define figure size\n",
    "    plt.figure(figsize=(16, 4))\n",
    "\n",
    "    # histogram\n",
    "    plt.subplot(1, 3, 1)\n",
    "    sns.histplot(df[variable], bins=30)\n",
    "    plt.title('Histogram')\n",
    "\n",
    "    # Q-Q plot\n",
    "    plt.subplot(1, 3, 2)\n",
    "    stats.probplot(df[variable], dist=\"norm\", plot=plt)\n",
    "    plt.ylabel('Variable quantiles')\n",
    "\n",
    "    # boxplot\n",
    "    plt.subplot(1, 3, 3)\n",
    "    sns.boxplot(y=df[variable])\n",
    "    plt.title('Boxplot')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnostic_plots(df_tmp_impute, 'GrossWeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnostic_plots(df_remov_outliers, 'GrossWeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No missing data\n",
    "df_remov_outliers.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving a copy of cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned_data = df_remov_outliers.copy()\n",
    "df_cleaned_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can count per day how many packets were sent for every postal code by aggregating.\n",
    "# \n",
    "data = df_cleaned_data.groupby(['DepartureDate','FromZone'],as_index = False).agg({'GrossWeight':'sum','ChargeWeight':'sum','Volume':'sum','NumberOfPieces':'sum','CostTotalAmount':'sum','FromZone':'count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.rename(columns = {'FromZone':'Count'}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separate Target Variable and Predictor Variables\n",
    "`Number Of Pieces` also as a predictor alongwith the `weight`, `cost`, `vol`. Reasoning is Higher number of pieces, higher weight, higher vol, and lower cost might result in higher NumerOfTransports the company has to arrange. `NumerOfTransports` is important because company needs more Drivers as a consequence for the packages.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Target = ['Count']\n",
    "Predictors=['GrossWeight', 'ChargeWeight', 'Volume', 'NumberOfPieces', 'CostTotalAmount']\n",
    "\n",
    "X = data[Predictors].values\n",
    "y = data[Target].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sandardization of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Predictor_Scaler=StandardScaler()\n",
    "Target_Scaler=StandardScaler()\n",
    "\n",
    "# Storing the fit object for later reference\n",
    "Predictor_ScalerFit = Predictor_Scaler.fit(X)\n",
    "Target_ScalerFit = Target_Scaler.fit(y)\n",
    " \n",
    "# Generating the standardized values of X and y\n",
    "X = Predictor_ScalerFit.transform(X)\n",
    "y = Target_ScalerFit.transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting the data into training and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Quick sanity check with the shapes of Training and testing datasets\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Creating a Ann Model to train our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create ANN model\n",
    "model = Sequential()\n",
    " \n",
    "# Defining the Input layer \n",
    "model.add(Dense(units=5, input_dim=5, kernel_initializer='normal', activation='relu'))\n",
    " \n",
    "# Defining the Second layer of the model\n",
    "model.add(Dense(units=5, kernel_initializer='normal', activation='tanh'))\n",
    " \n",
    "# The output neuron is a single fully connected node \n",
    "# Since we will be predicting a single number\n",
    "model.add(Dense(1, kernel_initializer='normal'))\n",
    " \n",
    "# Compiling the model\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    " \n",
    "# Fitting the ANN to the Training set\n",
    "model.fit(X_train, y_train ,batch_size = 20, epochs = 50, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating Predictions on testing data\n",
    "test_pred = model.predict(X_test)\n",
    "\n",
    "# Generating Predictions on training data\n",
    "train_pred = model.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 RandomForest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's build a machine learning model \n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Change max_samples value\n",
    "model_rfr = RandomForestRegressor(n_jobs=-1,\n",
    "                              random_state=42,\n",
    "                              max_samples=10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model_rfr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 Hyerparameter tuning with RandomizedSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Different RandomForestRegressor hyperparameters\n",
    "rf_grid = {\"n_estimators\": np.arange(10, 100, 10),\n",
    "           \"max_depth\": [None, 3, 5, 10],\n",
    "           \"min_samples_split\": np.arange(2, 20, 2),\n",
    "           \"min_samples_leaf\": np.arange(1, 20, 2),\n",
    "           \"max_features\": [0.5, 1, \"sqrt\", \"auto\"],\n",
    "           \"max_samples\": [10000]}\n",
    "\n",
    "# Instantiate RandomizedSearchCV model\n",
    "rs_model = RandomizedSearchCV(RandomForestRegressor(n_jobs=-1,\n",
    "                                                    random_state=42),\n",
    "                              param_distributions=rf_grid,\n",
    "                              n_iter=2,\n",
    "                              cv=5,\n",
    "                              verbose=True)\n",
    "\n",
    "# Fit the RandomizedSearchCV model\n",
    "rs_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best model hyperparameters\n",
    "rs_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Most ideal hyperparamters\n",
    "ideal_model = RandomForestRegressor(n_estimators=20,\n",
    "                                    min_samples_leaf=7,\n",
    "                                    min_samples_split=4,\n",
    "                                    max_features='auto',\n",
    "                                    n_jobs=-1,\n",
    "                                    max_samples=None,\n",
    "                                    random_state=42) # random state so our results are reproducible\n",
    "\n",
    "# Fit the ideal model\n",
    "ideal_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluating a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Model Performance on Train & Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to evaluate model on a few different levels\n",
    "def show_scores(model):\n",
    "    train_preds = model.predict(X_train)\n",
    "    test_preds = model.predict(X_test)\n",
    "    scores = {\"MAE for Train Data is: \": mean_absolute_error(y_train, train_preds),\n",
    "              \"MAE for Test Data is: \": mean_absolute_error(y_test, test_preds),\n",
    "              \"MSE for Train Data is: \": mean_squared_error(y_train, train_preds),\n",
    "              \"MSE for Test Data is: \": mean_squared_error(y_test, test_preds),\n",
    "              \"R2 Score for Train Data is\": r2_score(y_train, train_preds)*100,\n",
    "              \"R2 Score for Test Data is\": r2_score(y_test, test_preds)*100}\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1 ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_scores(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion** : Our Model performed very well and was able to give a R2 Score of 99% which is quite Decent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2 RandomForest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_scores(model_rfr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3 Hyerparameter tuning with RandomizedSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the RandomizedSearch model\n",
    "show_scores(rs_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train a model with the best hyperparamters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scores for ideal_model (trained on all the data)\n",
    "show_scores(ideal_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make predictions on test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test dataset\n",
    "test_preds = ideal_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find feature importance of our best model\n",
    "ideal_model.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Result\n",
    "\n",
    "\n",
    "| Ml/DL      | R2 Score (train/test)% | MAE (train/test)     | MSE (train/test)     |\n",
    "| :---        |    :----:   |          ---: |          ---: |\n",
    "| Neural Network (ANN)     | 99.78/99.80      | 0.0080/0.0073   |0.0022/0.0014\n",
    "| Random forest   | 99.9753/99.9744      | 0.1190/0.118     |7.171/5.226\n",
    "| Random forest with best Hyperparameter   | 99.63/99.46       | 0.4366/0.397      | 106.18/108.45\n",
    "\n",
    "\n",
    "\n",
    "**Conclusion** : All the models performed very well and was able to give a R2 Score of 99% which is quite Decent, but Random forest is the best model for this case!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance\n",
    "\n",
    "Feature selection is a critical step for most data science projects as it enables the models to train faster, reduces the complexity and makes it easier to interpret. It has the potential to improve model performance and reduce the problem of overfitting if the optimal set of features are chosen. In our data scince assigment, we don't this step because the data is not too large and it's quite easy to find what the most relevant columns in the dataset.\n",
    "\n",
    "The next step is to perform featue selection using the following techniques:\n",
    "\n",
    "- Select feature importance from random forest\n",
    "- Select the features identified by Lasso regression\n",
    "- Select features based on absolute value of beta coefficients of features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DataFrame\n",
    "X_df = data[Predictors]\n",
    "y_df = data[Target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_df, y_df, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for plotting feature importance\n",
    "def plot_features(columns, importances, n=20):\n",
    "    df = (pd.DataFrame({\"features\": columns,\n",
    "                        \"feature_importances\": importances})\n",
    "          .sort_values(\"feature_importances\", ascending=False)\n",
    "          .reset_index(drop=True))\n",
    "    \n",
    "    # Plot the dataframe\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.barh(df[\"features\"][:n], df[\"feature_importances\"][:20])\n",
    "    ax.set_ylabel(\"Features\")\n",
    "    ax.set_xlabel(\"Feature importance\")\n",
    "    ax.invert_yaxis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_features(X_train.columns, ideal_model.feature_importances_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 1: Variable Importance from Random Forest\n",
    "Random forests consist of multiple decision trees, each of them built over a random sample of the observations from the dataset and a random sample of the features. This random selection guarantees that the trees are not correlated and therefore less susceptible to over-fitting. For forecasting exercises, we use variable importance feature of random forest which measures how much the accuracy decreases when a variable is excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.Select the top n features based on feature importance from random forest\n",
    "import random\n",
    "np.random.seed(10)\n",
    "\n",
    "# define the model\n",
    "model = RandomForestRegressor(random_state = random.seed(10))\n",
    "# fit the model\n",
    "model.fit(X_df, y_df)\n",
    "\n",
    "# get importance\n",
    "features = X_df\n",
    "importances = model.feature_importances_\n",
    "indices = np.argsort(importances)\n",
    "\n",
    "feat_importances = pd.Series(model.feature_importances_, index=X_df.columns)\n",
    "feat_importances.nlargest(30).plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final Features from Random Forest (Select Features with highest feature importance)\n",
    "rf_top_features = pd.DataFrame(feat_importances.nlargest(4)).axes[0].tolist()\n",
    "rf_top_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 2: L1 regularisation using Lasso regression\n",
    "Lasso or L1 regularisation is based on the property that is able to shrink some of the coefficients in a linear regression to zero. Therefore, such features can be removed from the model. This is another example of an embedded method of feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso, LassoCV\n",
    "\n",
    "np.random.seed(10)\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "estimator = LassoCV(cv=5, normalize=True)\n",
    "\n",
    "sfm = SelectFromModel(estimator, prefit=False, norm_order=1, max_features=None)\n",
    "\n",
    "sfm.fit(X_df, y_df)\n",
    "\n",
    "feature_idx = sfm.get_support()\n",
    "Lasso_features = X_df.columns[feature_idx].tolist()\n",
    "Lasso_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Method 3: Beta Coefficients\n",
    "The absolute value of the coefficients of a standardized regression, also known as beta coefficients, can be considered a proxy for feature importance. This is a type of filter method of feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "#4.Perform recursive feature selection and use cross validation to identify the best number of features\n",
    "#Feature ranking with recursive feature elimination and cross-validated selection of the best number of features\n",
    "sr_reg = LinearRegression(fit_intercept = False).fit(X, y)\n",
    "coef_table = pd.DataFrame(list(X_df.columns)).copy()\n",
    "coef_table.insert(len(coef_table.columns),\"Coefs\",sr_reg.coef_.transpose())\n",
    "coef_table = coef_table.iloc[coef_table.Coefs.abs().argsort()] \n",
    "\n",
    "\n",
    "sr_data2 = coef_table.tail(10)\n",
    "sr_top_features = sr_data2.iloc[:,0].tolist()\n",
    "sr_top_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combining Feature Selection Methods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining features from all the models\n",
    "\n",
    "combined_feature_list = sr_top_features + Lasso_features + rf_top_features\n",
    "\n",
    "combined_feature = {x:combined_feature_list.count(x) for x in combined_feature_list}\n",
    "combined_feature_data = pd.DataFrame.from_dict(combined_feature,orient='index')\n",
    "\n",
    "combined_feature_data.rename(columns={ combined_feature_data.columns[0]: \"number_of_models\" }, inplace = True)\n",
    "\n",
    "\n",
    "combined_feature_data = combined_feature_data.sort_values(['number_of_models'], ascending=[False])\n",
    "\n",
    "combined_feature_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final Features: features which were selected in at least 3 models\n",
    "\n",
    "combined_feature_data = combined_feature_data.loc[combined_feature_data['number_of_models'] > 2]\n",
    "final_features = combined_feature_data.axes[0].tolist()\n",
    "final_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Obviously, the number of Pieces is the most important feature that affects the number of transport**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2 : Data Analytics\n",
    "\n",
    "## 1. Problem defition\n",
    "\n",
    "> Do a data exploration of several transport KPIs as a function of space and time, and derive some data-driven insights that may improve business decisions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaned_data.sort_values(by=[\"DepartureDate\"], inplace=True, ascending=True)\n",
    "df_cleaned_data['DepartureDate'] = df_cleaned_data['DepartureDate'].apply(pd.to_datetime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add datetime parameters for DepartureDate column\n",
    "\n",
    "Why?\n",
    "\n",
    "So we can enrich our dataset with as much information as possible.\n",
    "\n",
    "Because we imported the data using `read_csv()` and we asked pandas to parse the dates using `parase_dates=[\"DepartureDate\"]`, we can now access the [different datetime attributes](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DatetimeIndex.html) of the `DepartureDate` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add datetime parameters for saledate\n",
    "df_cleaned_data[\"DepartureDate_Year\"] = df_cleaned_data.DepartureDate.dt.year\n",
    "df_cleaned_data[\"DepartureDate_Month\"] = df_cleaned_data.DepartureDate.dt.month_name()\n",
    "df_cleaned_data[\"DepartureDate_Dayofweek\"] = df_cleaned_data.DepartureDate.dt.dayofweek\n",
    "df_cleaned_data[\"DepartureDate_Dayofyear\"] = df_cleaned_data.DepartureDate.dt.dayofyear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned_data.DepartureDate.min(), df_cleaned_data.DepartureDate.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Who are the top 5 Senders..?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned_data['FromZone'] = df_cleaned_data['FromZone'].apply(lambda x : x[:5])\n",
    "df_cleaned_data['ToZone'] = df_cleaned_data['ToZone'].apply(lambda x : x[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the Predictions \n",
    "sns.set(rc={'figure.figsize':(9.7,6.27)})\n",
    "sns.set_style(\"ticks\")\n",
    "\n",
    "ax = sns.countplot(y=df_cleaned_data['FromZone'], order=df_cleaned_data['FromZone'].value_counts().iloc[:5].index)\n",
    "ax.tick_params(axis='y', length=0)\n",
    "plt.xlabel(\"\", size=12)\n",
    "plt.ylabel(\"Sender\", size=12)\n",
    "plt.title(\"Top 5 Senders\", size=15)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top Five Senders are Clearly `50464`, `55652`, `19560`, `63346`, `43437`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Who are the top 5 Receivers..?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.countplot(x = df_cleaned_data['ToZone'], order = df_cleaned_data['ToZone'].value_counts().iloc[:5].index)\n",
    "ax.tick_params(axis='y', length = 0)\n",
    "plt.xlabel(\"Receiver\", size = 12)\n",
    "plt.ylabel(\"\", size = 12)\n",
    "plt.title(\"Top 5 Receivers\", size = 15)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top Five Receivers are Clearly `50464`, `16979`, `26036`, `18334`, `11121`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What's the top 5 Busiest Routes..?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned_data['Route'] = df_cleaned_data['FromZone'] + \"-\" + df_cleaned_data['ToZone']\n",
    "\n",
    "ax = sns.countplot(y=df_cleaned_data['Route'], order=df_cleaned_data['Route'].value_counts().iloc[:5].index)\n",
    "ax.tick_params(axis='y', length=0)\n",
    "plt.xlabel(\"\", size=12)\n",
    "plt.ylabel(\"Routes\", size=12)\n",
    "plt.title(\"Top 5 Busiest Routes\", size=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Top Busiest Route is with `50464` as the sender and `11121` as the Receiver**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How many Total Routes are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Total Routes are 42120\n",
    "df_cleaned_data['Route'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Which Month is Busiest for the Busiest Route..?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Busy_Route = df_cleaned_data[df_cleaned_data['Route']=='50464-11121']\n",
    "Busy_Route['Month'] = Busy_Route['DepartureDate'].dt.month_name()\n",
    "Busy_Route = Busy_Route.groupby(['Month','Route'],as_index = False).agg({'Route':'count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.bar(Busy_Route, x='Month', y='Route',  title=\"Busiest Month for the Busiest Route\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Busiest Months for the Busiest Route '50464 - 11121' are `July` and `August` with July having 536 Routes and August having 535 Route"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Which cities are the most Place of Destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Innrikes_Paket_kost['PlaceOfDestination'].value_counts().iloc[:5].index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "- Busiest routes will the company to make more resources available. Recognizing the business loads always help attract more.\n",
    "\n",
    "- Any transport business has different requirements for loading and unloading side. Recignizing important postal codes on broadcasting n receiving end is important\n",
    "\n",
    "- Company can do a Root Cause analysis or identify busy months in certain countries to aid with extra resources. Reasons for such peaks could be some special months where people send gifts, cards or tax filing end dates etc. Depends on many possible reason.\n",
    "\n",
    "- `STOCKHOLM` is the most destinated city in Sweden, which leads to more pollution than other cities!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3 Statistics\n",
    "\n",
    "> Q: A warehouse has a fixed operating cost of 1000 euros per day. For every truck it ships, it has an additional cost of 100 euros, but also a revenue after tax of 400 euros. The number of truck shipment orders the warehouse receives every day is modelled by a Poisson distribution with lambda = 4. Due to limited capacity in trucks, space and personnel, the maximum shipments the warehouse is able to deliver every given day is 8. What is the expected profit or loss in a month of 30 days?\n",
    "\n",
    ">A: Given that we have Poisson distribution we can calculate the:\n",
    "\n",
    "Given the equation:\n",
    "$$\n",
    "P(x) = \\frac{{e^{-\\lambda}} \\cdot {\\lambda^x}}{x!}\n",
    "$$\n",
    "\n",
    "\n",
    "-  **Probability of getting 8 trucks served per day**: `0.0298` \n",
    "- **If we serve 8 trucks a day we will get** `8x30 =240` **trucks per month**. We can find the expected number of trucks served per month: number of trucks served per month x probability.\n",
    "-   Hence, **the expected number of trucks served per month** is 8x30x0.0298 = `7` \n",
    "-   Costs for serving 7 trucks is : fixed costs per day ( 1 000) + costs for serving trucks ( 100 per truck) 30x1000 + 7x100 = 30 700\n",
    "-   ***Revenue* for serving trucks is** `7x400 = 2800`.\n",
    "-   **The expected *loss* is:** `2800 - 30 700 = -27 900`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('logistec')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "87ccbcd820c85ab21551c003710ca3342a305a01c89eb4274d7e9483918c7dd4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
